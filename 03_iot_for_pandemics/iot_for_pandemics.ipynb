{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IOT for Pandemics\n",
    "\n",
    "This notebook is part of [*Practical Data Science for IOT*](https://github.com/pablodecm/datalab_ml_iot) tutorial by Pablo de Castro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What can we do? (in addition to staying at home)\n",
    "\n",
    "Given the current COVID19 pandemic that is currently undergoing (this was initially\n",
    "written the 25th of March in Spain), it is worth thinking about possible technological\n",
    "solutions that could help improve or manage this crisis or future pandemics.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/data_science_diagram.png\" width=\"40%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Brainstorming Ideas\n",
    "\n",
    "Let's think about different solutions that could help with the crisis that use IOT technologies and data science, by iterating following this structure:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### WHY\n",
    "\n",
    "The actual problem or challenge that we are trying to solve. <br>\n",
    "Described in specific terms (not in general terms).  <br>\n",
    "Could be sub-problems of a larger problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### HOW\n",
    "\n",
    "Which technologies could be used to address the WHY. <br>\n",
    "How will these technologies interact. Estimate human and economical costs.  <br>\n",
    "Sketch the system components and how they play together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### WHAT\n",
    "\n",
    "Name or describe the solution. <br>\n",
    "Could it really address the WHY? <br>\n",
    "If yes, great! **Get feedback and/or try to build a PoC!** <br>\n",
    "If not, do not worry, keep iterating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Be bold!\n",
    "\n",
    "**Homework**: let's think individually or in groups about technological solutions of Data Science and IOT that could help with the COVID19 crisis following the previous structure.\n",
    "\n",
    "\n",
    "\n",
    "Send your ideas with this Google Form and we will discuss them tomorrow in class form:\n",
    "\n",
    "https://forms.gle/tZJh8hE3vLdwyyQW8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extra for tomorrow\n",
    "\n",
    "Go to  https://takeout.google.com/settings/takeout and download your own Location History data and keep it save, in the exercise tomorrow we will use our own location data. If possible, try to get the location data of someone else to study contract tracing between people.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/google_takeout.png\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example Idea\n",
    "\n",
    "\n",
    "### WHY\n",
    "\n",
    "SARS-CoV-2 virus is very contagious, due to the combined effect of a long incubation period, a large fraction of infected people only developing mild symptoms yet still being contagious and a high survivability in surfaces and air droplets.\n",
    "\n",
    "Uncontrolled transmission in the population can cause rapid growth with an associated large mortality within risk groups and easily overwhelm the health systems. Strong confinement and social distancing  seem the only effective way to stop the rapid spread if it is already out of control.\n",
    "\n",
    "While country-wide confinement is required in the short-term to stop the current transmission waves, it might not be sustainable long-term from a social and economical perspective (vaccine production at scale it is probably years away and treatments are likely to be of help but not a definite solution).\n",
    "\n",
    "**Without confinement even with massive testing it is really hard to track the virus transmission chain at scale, i.e. to find out who is likely to be infected by someone that has just tested positive. Better ways to trace transmissions between the population\n",
    "allow more directed testing campaign and containment by small group confinements.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### HOW\n",
    "\n",
    "Modern technology is likely to help with the problem of tracking the transmission chain. We need a way to register potential virus transmissions between people so it could be used to trace the graph of possible infections once someone has tested positive.\n",
    "\n",
    "This could be done with a powerful surveillance infrastructure, which is lacking in most countries and building it at the required scale is not possible or desired. Alternatively, the solution could be build based on people knowingly carrying personal devices, either voluntarily or enforced in public space until the crisis is controlled.\n",
    "\n",
    "Large fractions of the world population already own a internet-capable sensor-rich smartphone, and additional smartphone-like devices could be also provided at scale for those that do not.\n",
    "\n",
    "**A transmission trace system at national or multi-national scale can be based on location (e.g. cell phone tower triangulation or A-GPS logs), close distance between peers (e.g. ultrasound or Bluetooth) or a combination of all these technologies. Data already collected by companies could also be reused. A secure and escalable data collection and analysis infrastructure could be build rapidly in the cloud and managed by a trusted parties**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### WHAT\n",
    "\n",
    "A contact tracing system based on data collection by smartphones and other personal devices could be build in a short amount of time and could potentially address the problem of tracing the transmission chains, which in turn can be use to direct testing campaigns and enforce small group confinement, keeping most of the societal and economical activity intact.\n",
    "\n",
    "While promising, there are also unexplored concern regarding data protection and privacy, the different capabilities of the technologies mentioned, whether it could be enforced or a voluntary usage would suffice and how to put together the organisational, human and economical resources for creating an effective solution in a short time.\n",
    "\n",
    "**The best way to solve some of these uncertainties is to iteratively build proof of concept (PoC) examples of the system. We are gonna do a basic PoC of a tracing system based on Google Location history in the rest of this document.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Google Location History PoC\n",
    "\n",
    "For this part we are gonna use you own Google Location history (and optionally someone elses that has given acce you theirs).\n",
    "\n",
    "First step is to copy your own location history to the folder `google_location_history_data` in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrth google_location_history_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "loc_data_dir = Path(\"google_location_history_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_list = list(loc_data_dir.glob(\"*.zip\"))\n",
    "\n",
    "zip_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile(list(loc_data_dir.glob(\"*.zip\"))[1])\n",
    "zf.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_file_names = ['Takeout/Historial de ubicaciones/Historial de ubicaciones.json',\n",
    "                   'Takeout/Location History/Location History.json']\n",
    "\n",
    "# this is an example of how to read from a zipped file\n",
    "# without decompressing\n",
    "def load_dataframe_from_file(file_path: Path):\n",
    "    \n",
    "    zip_file = zipfile.ZipFile(file_path)\n",
    "    for file_name in zip_file.namelist():\n",
    "        if file_name in good_file_names:\n",
    "            return pd.read_json(io.BytesIO(zip_file.read(file_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the data from one filter to check that works\n",
    "df = load_dataframe_from_file(zip_file_list[0])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0,\"locations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is from the Google Location History guide:\n",
    "\n",
    "```\n",
    "The JSON Location History file describes device location signals and associated metadata collected while you were opted into Location History which you have not subsequently deleted.\n",
    "\n",
    "locations: All location records.\n",
    "timestampMs(int64): Timestamp (UTC) in milliseconds for the recorded location.\n",
    "latitudeE7(int32): The latitude value of the location in E7 format (degrees multiplied by 10**7 and rounded to the nearest integer).\n",
    "longitudeE7(int32): The longitude value of the location in E7 format (degrees multiplied by 10**7 and rounded to the nearest integer).\n",
    "accuracy(int32): Approximate location accuracy radius in meters.\n",
    "velocity(int32): Speed in meters per second.\n",
    "heading(int32): Degrees east of true north.\n",
    "altitude(int32): Meters above the WGS84 reference ellipsoid.\n",
    "verticalAccuracy(int32): Vertical accuracy calculated in meters.\n",
    "activity: Information about the activity at the location.\n",
    "timestampMs(int64): Timestamp (UTC) in milliseconds for the recorded activity.\n",
    "type: Description of the activity type.\n",
    "confidence(int32): Confidence associated with the specified activity type.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to extract the different field in the dict\n",
    "# to create useful analysis variables\n",
    "\n",
    "def timestamp_from_location_dict(location_dict: Dict,\n",
    "                                 field_name: str= \"timestampMs\"\n",
    "                                ) -> pd.Timestamp:\n",
    "    timestamp_sec = int(location_dict[field_name])/1000.\n",
    "    return pd.Timestamp.fromtimestamp(timestamp_sec)\n",
    "\n",
    "def coordinate_from_location_dict(location_dict: Dict,\n",
    "                                  cood_name: str) -> pd.np.float32:\n",
    "    return location_dict[cood_name]/10.**7\n",
    "\n",
    "def format_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    if \"locations\" in df:\n",
    "        df[\"timestamp\"] = df[\"locations\"].map(\n",
    "            timestamp_from_location_dict)\n",
    "        df[\"latitude\"] =  df[\"locations\"].map(\n",
    "            lambda l_d: l_d['latitudeE7']/10.**7)\n",
    "        df[\"longitude\"] =  df[\"locations\"].map(\n",
    "            lambda l_d: l_d['longitudeE7']/10.**7)\n",
    "\n",
    "        df[\"accuracy\"] = df[\"locations\"].map(\n",
    "            lambda l_d: l_d['accuracy'])\n",
    "\n",
    "        del df[\"locations\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets test if the formating worked\n",
    "format_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_dfs = {}\n",
    "\n",
    "for zip_file in zip_file_list:\n",
    "    df = load_dataframe_from_file(zip_file)\n",
    "    available_dfs[zip_file.stem] = format_dataframe(df)\n",
    "    print(f\"{zip_file.stem}\")\n",
    "    print(f\"  - latest data {df.timestamp.max()}\")\n",
    "    print(f\"  - n entries total {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out not accurate data\n",
    "min_accuracy = 30\n",
    "for name in available_dfs:\n",
    "    df = available_dfs[name]\n",
    "    low_acurracy_filter = df.accuracy > min_accuracy\n",
    "    n_entries_removed = low_acurracy_filter.sum()\n",
    "    frac_entries_removed = n_entries_removed/float(len(df))\n",
    "    available_dfs[name] = df.loc[~low_acurracy_filter]\n",
    "    print(f\"{name} removed {n_entries_removed} ({frac_entries_removed}) entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"Histograms of accuracy\")\n",
    "\n",
    "bins =  np.linspace(0, min_accuracy, 16)\n",
    "for name, df in available_dfs.items():\n",
    "   \n",
    "    ax.hist(df.accuracy, bins=bins,density=True, histtype='step')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.timestamp.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use strftime because the weekofyear starts on Sunday\n",
    "df[\"weekofyear\"] = df.timestamp.dt.strftime('%W').astype(int)\n",
    "df[\"year\"] = df.timestamp.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many datapoints per year\n",
    "df.groupby([\"year\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also groupby two columns\n",
    "df.groupby([\"year\", \"weekofyear\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trick to generate the location history of many individuals\n",
    "# from only a few\n",
    "\n",
    "# we will make all the data correspond to the week starting\n",
    "\n",
    "fake_start_week = pd.to_datetime('2020121', format='%Y%W%w')\n",
    "\n",
    "many_df = {}\n",
    "for name in available_dfs:\n",
    "    \n",
    "    df = available_dfs[name]\n",
    "    df[\"weekofyear\"] = df.timestamp.dt.strftime('%W').astype(int)\n",
    "    df[\"year\"] = df.timestamp.dt.year\n",
    "\n",
    "    grouped_df = df.groupby([\"year\", \"weekofyear\"])\n",
    "    for keys, group_df in grouped_df:\n",
    "        year, weekofyear = keys\n",
    "        \n",
    "        # shift datetime so all the data start on fake_week_starts\n",
    "        date_str = f\"{year:04d}{weekofyear:02d}\"\n",
    "        week_date = pd.to_datetime(date_str + '1', format='%Y%W%w')\n",
    "        shift = fake_start_week - week_date\n",
    "        new_df =  group_df.copy()\n",
    "        new_df[\"timestamp\"] = group_df.timestamp + shift\n",
    "        \n",
    "        # create new dataframe (use timestamp as index)\n",
    "        new_name =  f\"{name}_{year}_{weekofyear}\"\n",
    "        del new_df[\"weekofyear\"]\n",
    "        del new_df[\"year\"]\n",
    "        many_df[new_name] = new_df.set_index(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have now the data equivalent to several people during\n",
    "# last week\n",
    "len(many_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it might be easier to use ordered lists\n",
    "many_df_list = list(many_df.values())\n",
    "many_df_names = list(many_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "import itertools\n",
    "\n",
    "cycle_colors = itertools.cycle(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "coord_names = [\"latitude\", \"longitude\"]\n",
    "\n",
    "location = (41.6523, -4.7245)\n",
    "\n",
    "example_loc_hists = random.sample(many_df_list, 10)\n",
    "\n",
    "m = folium.Map(location=location,\n",
    "               zoom_start=1)\n",
    "\n",
    "for example_df, color in zip(example_loc_hists,\n",
    "                             cycle_colors):\n",
    "    \n",
    "    points = example_df[coord_names].values\n",
    "    folium.PolyLine(points, color=color).add_to(m)\n",
    "    \n",
    "    print(example_df.index.min(), example_df.index.max())\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = example_loc_hists[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resample = df.resample('1min').mean()\n",
    "df_interp = df_resample.interpolate('time')\n",
    "df_interp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can apply the interpolation to all the dataframes\n",
    "# to unify the treatment\n",
    "\n",
    "# to keep the detailed data we are only going only to consider\n",
    "# cases with more than 1000 row\n",
    "min_n_rows = 1000\n",
    "\n",
    "formated_df_dict = {}\n",
    "for name, df in many_df.items():\n",
    "    if len(df) > min_n_rows:\n",
    "        resampled_df = df.resample('1min').mean()\n",
    "        \n",
    "        df_resample = df.resample('1min').mean()\n",
    "        df_interp = df_resample.interpolate('time')\n",
    "        \n",
    "        formated_df_dict[name] = df_interp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_df_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create single single dataframe with index level per person\n",
    "single_df = pd.concat(formated_df_dict, names=[\"person_id\"])\n",
    "single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a more useful grouping\n",
    "time_single_df = single_df.swaplevel().sort_index()\n",
    "time_single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_time = \"2020-03-25 19:00:00\"\n",
    "example_time_df = time_single_df.loc[example_time, coord_names]\n",
    "example_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine_vector, Unit\n",
    "\n",
    "coord_arr = example_time_df.values\n",
    "dist_matrix = haversine_vector(coord_arr[:,:,np.newaxis], coord_arr, unit=Unit.METERS)\n",
    "upper_diag_filter = np.triu(np.ones_like(dist_matrix, dtype=np.bool), k=1)\n",
    "closer_than_matrix = dist_matrix < 60.0\n",
    "print(\"possible pairs: \", np.sum(upper_diag_filter))\n",
    "closer_pair_mask = upper_diag_filter & closer_than_matrix\n",
    "print(\"closer pairs: \", np.sum(closer_pair_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = np.array(np.where(closer_pair_mask)).T\n",
    "\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24,24))\n",
    "gr = nx.Graph()\n",
    "gr.add_edges_from(edges)\n",
    "pos=nx.spring_layout(gr, k=0.3)\n",
    "nx.draw_networkx(gr, pos=pos, node_size=200,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "# carculate the centroid\n",
    "centroid_coord = (coord_arr[edges[:,0]] + coord_arr[edges[:,1]])/2.\n",
    "\n",
    "m = folium.Map(location=location,\n",
    "               zoom_start=6) \n",
    "\n",
    "HeatMap(data=centroid_coord.tolist()).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now suppose that someone has tested positive on the 29th\n",
    "# find who else could he/she have infected last week by being in contact\n",
    "# and where it could have happened\n",
    "\n",
    "infected_person = random.sample(list(time_single_df.index.get_level_values(\"person_id\").unique()),1)\n",
    "print(infected_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "rise": {
   "theme": "black"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
